{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1V6BDRB12etHVHw6WkGAQAHz6Vb5UmdNo",
      "authorship_tag": "ABX9TyNzXd9H3+/GlU9nb9zHZRag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marrej/ML-projects/blob/main/DeepQLearning_With_Pacman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q learning on Atari games\n",
        "\n",
        "This notebook tries out the basics of Q learning (implementing the NN network & algorithm from scrath) on the Atari games (e.g. Space invaders)."
      ],
      "metadata": {
        "id": "ncdaS_DTTf2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gymnasium gymnasium[atari] gymnasium[accept-rom-license]\n",
        "!pip3 install imageio\n",
        "!pip3 install stable_baselines3\n",
        "!pip3 install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFDmOEGAn0f4",
        "outputId": "16129c67-4b6a-4dcd-d02b-e5d7d68f08a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium)\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium)\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (4.66.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium)\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium) (6.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2023.11.17)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=dcf6764b45352bd82f1685bc07c11eca334bf0b68bc7f8b8243dbc46a0375d8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ale_py==0.8.1"
      ],
      "metadata": {
        "id": "U0xmBJ15EQvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3.common.atari_wrappers import (\n",
        "    ClipRewardEnv,\n",
        "    EpisodicLifeEnv,\n",
        "    FireResetEnv,\n",
        "    MaxAndSkipEnv,\n",
        "    NoopResetEnv,\n",
        ")\n",
        "\n",
        "def make_env():\n",
        "  def thunk():\n",
        "    # Use grayscale so that we reduce the amount of data in the net\n",
        "    game_env = gym.make(\"MsPacmanNoFrameskip-v4\", obs_type=\"grayscale\", render_mode=\"rgb_array\")\n",
        "    # Should resize to reduce the amount of consumed space\n",
        "    game_env = gym.wrappers.ResizeObservation(game_env, (84, 84))\n",
        "    game_env = ClipRewardEnv(game_env)\n",
        "    # This bugs the game_env\n",
        "    game_env = NoopResetEnv(game_env, noop_max=30)\n",
        "    game_env = MaxAndSkipEnv(game_env, skip=4)\n",
        "    game_env = EpisodicLifeEnv(game_env)\n",
        "    # # Use 4 image states\n",
        "    game_env = gym.wrappers.FrameStack(game_env, 4)\n",
        "    game_env = gym.wrappers.RecordEpisodeStatistics(game_env)\n",
        "\n",
        "    return game_env\n",
        "  return thunk\n",
        "\n",
        "game_env = make_env()()"
      ],
      "metadata": {
        "id": "egC78IYyUADa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "game_env.reset()\n",
        "Image.fromarray(np.uint8(game_env.render()))"
      ],
      "metadata": {
        "id": "5XUrspPtqM8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import random\n",
        "\n",
        "def record_video(env, QAgent, out_directory, fps=15):\n",
        "    \"\"\"\n",
        "    Generate a replay video of the agent\n",
        "    :param env\n",
        "    :param QAgent: QAgent of our agent\n",
        "    :param out_directory\n",
        "    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    for i in range(3):\n",
        "      terminated = False\n",
        "      truncated = False\n",
        "      state, info = env.reset(seed=random.randint(0, 500))\n",
        "\n",
        "      img = env.render()\n",
        "      images.append(img)\n",
        "      while not terminated or truncated:\n",
        "          # Take the action (index) that have the maximum expected future reward given that state\n",
        "          action = QAgent.get_action(np.array(state), epsilon=0)\n",
        "\n",
        "          state, reward, terminated, truncated, info = env.step(\n",
        "              action\n",
        "          )\n",
        "          # We directly put next_state = state for recording logic\n",
        "          img = env.render()\n",
        "          images.append(img)\n",
        "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "\n",
        "class DummyQAgent():\n",
        "  def get_action(self, state, epsilon):\n",
        "    return random.randint(0, 8)\n",
        "\n",
        "# Generates a dummy video for a random traversal\n",
        "ag = DummyQAgent()\n",
        "record_video(make_env()(), ag, \"replay.mp4\")"
      ],
      "metadata": {
        "id": "2o3rzmIXq7_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount G drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ebE4ocwFOpYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN definition"
      ],
      "metadata": {
        "id": "eeHyjG7rT_M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class QNet(nn.Module):\n",
        "  def __init__(self, env):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        ## We use 4 images as a state, but could increase this\n",
        "        nn.Conv2d(4, 32, 8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, 4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, 3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3136, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, env.single_action_space.n),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x / 255.0)\n",
        "\n",
        "class DeepQLearning():\n",
        "  def __init__(self, is_cuda=False):\n",
        "    \"\"\"\n",
        "    params\n",
        "    is_cuda = False: sets the cuda as main processing device if available\n",
        "    \"\"\"\n",
        "    self.is_cuda = is_cuda\n",
        "\n",
        "  def init_nets(self, env):\n",
        "    # initialize a training model (target)\n",
        "    # initialize a Qmodel\n",
        "    self.target_net = QNet(env).to(self.get_device())\n",
        "    self.q_net = QNet(env).to(self.get_device())\n",
        "    self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "  def get_epsilon(self, duration, time_step, e_min=0.05, e_max=1.0,):\n",
        "    # Calculate the time step based on the duration\n",
        "    slope = (e_max - e_min)/duration\n",
        "    return max(e_min, e_max - slope*time_step)\n",
        "\n",
        "  def get_device(self):\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() and self.is_cuda else \"cpu\")\n",
        "\n",
        "  def fine_tune(self,\n",
        "            q_net,\n",
        "            ens,\n",
        "            start_training_after_steps=20000,\n",
        "            max_epsilon=1.0,\n",
        "            min_epsilon=0.05,\n",
        "            max_steps=3000000,\n",
        "            batch_size=32,\n",
        "            learning_rate=0.0001,\n",
        "            # At which timesteps do we update the target network\n",
        "            target_network_frequency_update=5000,\n",
        "            # Update rate of the target network\n",
        "            tau = 0.9,\n",
        "            gamma=0.99,\n",
        "            # how often should we  run eval\n",
        "            eval_frequency=50000,\n",
        "            checkpoint_frequency=50000,\n",
        "            # How often should we train\n",
        "            training_frequency=4):\n",
        "    print('Fine tuning')\n",
        "    env = gym.vector.SyncVectorEnv(ens)\n",
        "\n",
        "    # Initialize new nets\n",
        "    self.target_net = QNet(env).to(self.get_device())\n",
        "    self.q_net = QNet(env).to(self.get_device())\n",
        "\n",
        "    # Copy params from the loaded net\n",
        "    # Maybe I should Initialize the target_net with a random value and then load from Q_net with a tau the same way as we do in training. So its not equal from the start, otherwise we hinder training?\n",
        "    self.q_net.load_state_dict(q_net.state_dict())\n",
        "    # Load the target_net as a Q_net progress\n",
        "    self.update_target_params(tau)\n",
        "\n",
        "    return self.train(ens,\n",
        "            start_training_after_steps=start_training_after_steps,\n",
        "            max_epsilon=max_epsilon,\n",
        "            min_epsilon=min_epsilon,\n",
        "            max_steps=max_steps,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            target_network_frequency_update=5000,\n",
        "            tau=tau,\n",
        "            gamma=gamma,\n",
        "            eval_frequency=eval_frequency,\n",
        "            checkpoint_frequency=checkpoint_frequency,\n",
        "            training_frequency=training_frequency)\n",
        "\n",
        "  def train(self,\n",
        "            ens,\n",
        "            start_training_after_steps=100000,\n",
        "            max_epsilon=1.0,\n",
        "            min_epsilon=0.05,\n",
        "            max_steps=5000000,\n",
        "            batch_size=32,\n",
        "            learning_rate=0.001,\n",
        "            # At which timesteps do we update the target network\n",
        "            target_network_frequency_update=5000,\n",
        "            # Update rate of the target network\n",
        "            tau = 0.9,\n",
        "            gamma=0.9,\n",
        "            # how often should we run eval\n",
        "            eval_frequency=50000,\n",
        "            checkpoint_frequency=50000,\n",
        "            # How often should we train\n",
        "            training_frequency=4,\n",
        "            fine_tuning=False,\n",
        "            preloaded_envs = None\n",
        "            ):\n",
        "    print('Training initialized')\n",
        "    if preloaded_envs == None:\n",
        "      env = gym.vector.SyncVectorEnv(ens)\n",
        "    else:\n",
        "      env = preloaded_envs\n",
        "\n",
        "    if fine_tuning == False:\n",
        "      self.init_nets(env)\n",
        "      print('Nets initialized')\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # initialize a ActionReplay\n",
        "\n",
        "    ## How do we store the rewards?\n",
        "    replay_buffer = ReplayBuffer(\n",
        "        n_envs=len(ens),\n",
        "        action_space=env.single_action_space,\n",
        "        observation_space=env.single_observation_space,\n",
        "        buffer_size=80000,\n",
        "        device=self.get_device(),\n",
        "        handle_timeout_termination=False\n",
        "    )\n",
        "\n",
        "    # History of loss/rewards through steps\n",
        "    history = []\n",
        "\n",
        "    # initialize the base state\n",
        "    observation, info = env.reset()\n",
        "\n",
        "    # do a Sample/Train cycle\n",
        "    for global_step in tqdm(range(max_steps)):\n",
        "\n",
        "      epsilon = self.get_epsilon(e_min=min_epsilon, e_max=max_epsilon, time_step=global_step, duration=max_steps)\n",
        "\n",
        "      ## Do n sampling cycles\n",
        "\n",
        "      action = self.get_epsilon_greedy_action(env, len(ens), observation, epsilon)\n",
        "      ### Sample action from the training model (usign the epsilon greedy policy)\n",
        "\n",
        "      next_observations, reward, termination, truncation, info = env.step(action)\n",
        "\n",
        "      ### Add the state,action,reward to the ReplayAction stack\n",
        "      replay_buffer.add(observation, next_observations.copy(), action, reward, termination, info)\n",
        "\n",
        "      # Use the next observation as base for next run\n",
        "      observation = next_observations\n",
        "\n",
        "\n",
        "      if global_step >= start_training_after_steps and global_step % training_frequency == 0:\n",
        "        ## Retrieve N random state sequences (j, j+n)\n",
        "        sequence = replay_buffer.sample(batch_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # grab the target values (will contain batchsize*amt of actions)\n",
        "          target_max, arg_max = self.target_net(sequence.next_observations).to(float).max(dim=1)\n",
        "          target_val = sequence.rewards.flatten().to(float) + gamma * target_max * (1.0 - sequence.dones.flatten().to(float))\n",
        "\n",
        "        ### Run a Qmodel to retrieve an action (with epislon greedy policy)\n",
        "        # gathers the actions that were executed from the observations and reduces the dimentions to (32 from 32x1)\n",
        "        # https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
        "        old_val = self.q_net(sequence.observations).gather(1, sequence.actions).squeeze()\n",
        "\n",
        "        # Converting the values to float to avoid problems with model being Float but values being double\n",
        "        # see https://stackoverflow.com/questions/67456368/pytorch-getting-runtimeerror-found-dtype-double-but-expected-float\n",
        "        loss = F.mse_loss(target_val.float(), old_val.float())\n",
        "\n",
        "        # calculate the loss and update the Q_net\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # So that both networks are in sync, or rather the Q network doesn play catch and mouse with the moving gradient of target, we need to update it in some timesteps\n",
        "        if global_step % target_network_frequency_update == 0:\n",
        "          self.update_target_params(tau)\n",
        "\n",
        "      # Do checkpoints while training\n",
        "      if global_step % checkpoint_frequency == 0 and global_step >= start_training_after_steps:\n",
        "        checkpoint_name = 'rl_checkpoint_4_agents___steps_at_{0}'\n",
        "        torch.save(self.q_net.state_dict(), checkpoint_name.format(global_step))\n",
        "\n",
        "      # If termination + truncation are all true than we should restart all envs\n",
        "      # if np.mean(termination+truncation) > 0.0:\n",
        "        # print(\"\\n!restarting at step {0}\".format(global_step))\n",
        "        # not needed due to episodic life from atari wrappers\n",
        "        # observation, info = env.reset()\n",
        "\n",
        "      # Run eval only after training the model\n",
        "      if global_step % eval_frequency == 0 and global_step >= start_training_after_steps:\n",
        "        mean_rewards, mse, max_rewards, avg_steps = eval(self.q_net, make_env(), eval_episodes=2, target_net=self.target_net)\n",
        "        history.append({\"step\": global_step, \"rewards\": mean_rewards, \"mse\": mse, \"avg_steps\": avg_steps, \"max_rewards\": max_rewards})\n",
        "        print('\\nStep: ', global_step, ', mean rewards: ', mean_rewards, ', mse:', mse, ', epsilon: ', epsilon, ', avg_steps: ',avg_steps,'\\n' )\n",
        "\n",
        "    return self.q_net, history\n",
        "\n",
        "  def update_target_params(self, tau):\n",
        "    target_params_copy = self.target_net.state_dict().copy()\n",
        "    for q_key, target_key in zip(self.q_net.state_dict(), self.target_net.state_dict()):\n",
        "      ## Use the state dict to pull all the  network weights & based on the keys we should be able to update it based on the tau\n",
        "      if q_key != target_key:\n",
        "        raise Exception(\"Sorry uneven nets, with unnasignable parameters\")\n",
        "\n",
        "      # Update the target network with a mixture of q_params + target_params (weight+bias), by using tau as the raio of qParams to targetParams\n",
        "      target_params_copy[q_key] = self.q_net.state_dict()[q_key] * tau + self.target_net.state_dict()[target_key] * (1.0 - tau)\n",
        "\n",
        "    # Update the target network parameters with the updated parameters\n",
        "    self.target_net.load_state_dict(target_params_copy)\n",
        "\n",
        "  def get_epsilon_greedy_action(self, env, env_n, state, epsilon):\n",
        "    if random.random() > epsilon:\n",
        "      # Question this piece. I am not fully sure that this is correct also for me. Althought he network seems to train\n",
        "      q_vals = self.q_net(torch.Tensor(state).to(self.get_device()))\n",
        "      return torch.argmax(q_vals, dim=1).cpu().numpy()\n",
        "    else:\n",
        "      # Generate a random action for each environment\n",
        "      return np.array([env.single_action_space.sample() for _ in range(env_n)])\n",
        "\n",
        "# Detached from the main object (generic functions)\n",
        "\n",
        "def get_greedy_action(q_net, observation):\n",
        "    q_val = q_net(torch.Tensor(observation).to('cpu').resize(1, 4, 84, 84)).argmax(dim=1)\n",
        "    return torch.argmax(q_val).cpu().numpy()\n",
        "\n",
        "# Gets the value of the best action\n",
        "def get_greedy_val(q_net, observation):\n",
        "    q_val = q_net(torch.Tensor(observation).to('cpu').resize(1, 4, 84, 84)).max(dim=1)\n",
        "    return q_val.values.cpu().detach().numpy()\n",
        "\n",
        "def eval(q_net, env_func, eval_episodes=100, max_eval_steps=100000, target_net=None, lives_in_episode=3, save_video=False):\n",
        "  env_to_eval = env_func()\n",
        "  rewards = []\n",
        "  loss = []\n",
        "  steps = []\n",
        "  max_rewards_per_episode = 0\n",
        "  for episode in tqdm(range(eval_episodes)):\n",
        "    images = []\n",
        "    episode_rewards = 0\n",
        "    episode_steps = 0\n",
        "    # Due to 3 lifes\n",
        "    for i in range(lives_in_episode):\n",
        "      observation, info = env_to_eval.reset()\n",
        "      step = 0\n",
        "      truncation = False\n",
        "      termination = False\n",
        "      life_rewards = []\n",
        "\n",
        "      img = env_to_eval.render()\n",
        "      images.append(img)\n",
        "\n",
        "      while truncation == False and termination == False and step < max_eval_steps:\n",
        "        action = q_net(torch.Tensor(observation).to('cpu').resize(1, 4, 84, 84)).argmax(dim=1)\n",
        "        next_observations, reward, termination, truncation, info = env_to_eval.step(action)\n",
        "\n",
        "        img = env_to_eval.render()\n",
        "        images.append(img)\n",
        "\n",
        "        life_rewards.append(np.array(reward).sum())\n",
        "\n",
        "        # if has target_net calculate errors\n",
        "        if target_net != None:\n",
        "          expected_action = get_greedy_val(target_net, observation)\n",
        "          used_action = get_greedy_val(q_net, observation)\n",
        "          with torch.no_grad():\n",
        "            squared_loss=((action - expected_action)**2).cpu().numpy()\n",
        "          loss.append(squared_loss)\n",
        "\n",
        "        observation = next_observations\n",
        "        step=step+1\n",
        "        episode_steps=episode_steps+1\n",
        "\n",
        "      episode_rewards = episode_rewards + np.array(life_rewards).sum()\n",
        "\n",
        "    steps.append(episode_steps)\n",
        "    rewards.append(episode_rewards)\n",
        "    max_rewards_per_episode = max(max_rewards_per_episode, np.array(episode_rewards).sum())\n",
        "\n",
        "    # render images conditionally\n",
        "    imageio.mimsave(\"eval_ep{0}.mp4\".format(episode), [np.array(img) for i, img in enumerate(images)], fps=20)\n",
        "  return np.array(rewards).mean(), np.array(loss).mean(), max_rewards_per_episode, np.array(steps).mean()\n",
        "\n",
        "agents = [make_env() for i in range(4)]\n",
        "trained_q, history = DeepQLearning(False).train(agents)"
      ],
      "metadata": {
        "id": "CP3Ll0WwT7eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and saving the checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference\n",
        "# torch.save(trained_q.state_dict(), 'rl_checkpoint_4_agents')\n",
        "\n",
        "eval_env_single = make_env()\n",
        "eval_env = gym.vector.SyncVectorEnv([eval_env_single])\n",
        "loaded_q = QNet(eval_env)\n",
        "loaded_q.load_state_dict(torch.load('rl_checkpoint_800k_steps'))"
      ],
      "metadata": {
        "id": "fJM5y2h9znaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agents = [make_env() for i in range(2)]\n",
        "trained_q, history = DeepQLearning(False).fine_tune(loaded_q, agents, max_epsilon=0.7)\n",
        "torch.save(trained_q.state_dict(), 'rl_checkpoint_2_agents_fine_tuned')"
      ],
      "metadata": {
        "id": "TVuJdfcY5Qj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lI8caRumbYKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval(loaded_q, eval_env_single, eval_episodes=2, max_eval_steps=1000000, save_video=True)"
      ],
      "metadata": {
        "id": "4WUEGiNCyGfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QAgent used for video recording\n",
        "class QAgent():\n",
        "  def __init__(self, q_net):\n",
        "    self.q_net = q_net\n",
        "\n",
        "  def get_action(self, state, epsilon):\n",
        "    return self.q_net(torch.Tensor(state).to('cpu').resize(1, 4, 84, 84)).argmax(dim=1)\n",
        "\n",
        "\n",
        "agent = QAgent(loaded_q)\n",
        "record_video(eval_env_single(), agent, \"replay.mp4\")"
      ],
      "metadata": {
        "id": "w5kHZmEVuu-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "The best performing agent was at 10k steps. So the problem maybe is not that the agents are not doing what they should, but maybe that I am using too few envs? E.g. I should be using hundreds of agents at the same time, or also my epsilon goes crazy, due to it being step related not slope related. Maybe It should become less aggresive based on the mean errors for a batch rather than being a constant slope?\n",
        "\n",
        "- Add it to the eval"
      ],
      "metadata": {
        "id": "rf_pq9ZJ4ELv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODOS:\n",
        "\n",
        "- Add evaluation in batches (which returns average rewards + video if requested)\n",
        "- rework the structure to support N environments (Whatever environments)\n",
        "- Add eval data that will be exported once training ends so we can plot steps, mean_error, rewards"
      ],
      "metadata": {
        "id": "8OyGOB9LpIJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zQP8ILtgvx2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UBw2sxHlTYuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blog post notes:\n",
        "\n",
        "- How do we do the epsilon? Is steps based correct?\n",
        "- How many agents should we do? More agents less global steps?\n",
        "- How does training sooner (sampling from the batch) affect the training? Do we first need more data to be sampled? Or do we just do it to save processing time on first N steps??\n",
        "  - Start training afte N steps should provide enough gap for the agents to experiment enough before learning & adjusting, and not oversampling on already learned. E.g. Batch of 32 is too much if we start learning after 100 steps\n",
        "- SHould we experiment with the restarting? Can that overfit? When should we restart? e.g. half environments dead?\n",
        "- Although MSE is improving wee don't se improvement in rewards (updating to use slope instead of step decay) -> Should we consider using only duration after we start training to use the learned steps??\n",
        "- DId I really make error in the Epsilon?! -> always double check it in eval rounds\n",
        "- Strive to understand the underlying pieces. Why are some properties updated in a simple way, why do other require slices.\n",
        "- validate. Don't just blindly copy and paste things/ or copy and rewrite. You need to get in and play around to get a better understanding\n",
        "- Added fine_tune method so we can continue from the last time that the training stopped -> This avoids dropouts in colab. What we could do as well is store the *Target_net* which would really allow to continue from where we left of. Interesting would be also to append the history so we can actually use it to continue observing the improvements"
      ],
      "metadata": {
        "id": "B_JQWHDn44Ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leader board:\n",
        "\n",
        "Steps | MaxScore | MeanScore | info\n",
        "\n",
        "840k  | 1140  | 75.0  | did direct training with lr 0.001\n",
        "300k finetune | | | did 300k directly with 0.001 then finetune for additional 5mil steps & increase actors to 20"
      ],
      "metadata": {
        "id": "ddB5AR7bXKZf"
      }
    }
  ]
}